{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the functions for the solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_functions as inp\n",
    "import tsp_routines\n",
    "from clustering.funcs import k_cluster\n",
    "from clustering.funcs import best_dropoff\n",
    "from input_functions.funcs import create_new_graph\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "cpus = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dict_entries(input_dict,nodemapper):\n",
    "    \"\"\"\n",
    "    maps the keys and values of the input dict using the nodemapper dict.\n",
    "    \"\"\"\n",
    "    output_dict = dict()\n",
    "    for key in input_dict.keys():\n",
    "        mapped_key = nodemapper[key]\n",
    "        output_dict.update({mapped_key:[]})\n",
    "        for vertex in input_dict[key]:\n",
    "            output_dict[mapped_key].append(nodemapper[vertex])\n",
    "    return output_dict\n",
    "\n",
    "def tsp_solution_to_path(G,tsp_route):\n",
    "    \"\"\"\n",
    "    converts the given tsp sequence to be followed by the car into a \n",
    "    path in the graph G\n",
    "    Input:\n",
    "    G - undirected weighted input graph\n",
    "    tsp_route - list of vertices specifying the route to be followed by the car\n",
    "    \"\"\"\n",
    "    prev = tsp_route[0]\n",
    "    final_path = []\n",
    "    final_path.append(prev)\n",
    "    for vertex in tsp_route[1:]:\n",
    "        path = nx.shortest_path(G,prev,vertex,weight='weight')\n",
    "        final_path += path[1:]\n",
    "        prev = vertex\n",
    "    return final_path\n",
    "\n",
    "def add_vertex_to_clusters(clusters,vertex):\n",
    "    \"\"\"\n",
    "    add the given vertex to each cluster.\n",
    "    Input:\n",
    "    clusters - dict where the keys are vertices which are cluster centers and the values are a list of \n",
    "                vertices belonging to this cluster\n",
    "    vertex - the vertex to be added to each list in `clusters`\n",
    "    \"\"\"\n",
    "    for key in clusters:\n",
    "        clusters[key].append(vertex)\n",
    "        \n",
    "def get_dropoff_vertices(G, clusters):\n",
    "    best_dropoffs = []\n",
    "    for key in clusters:\n",
    "        dropoff = best_dropoff(G,clusters[key])\n",
    "        best_dropoffs.append(dropoff)\n",
    "    return best_dropoffs\n",
    "        \n",
    "def solve_by_clustering(graph,homes,source,num_clusters):\n",
    "    \"\"\"\n",
    "    return the route to be followed by the car as it drops off TAs.\n",
    "    Inputs:\n",
    "    graph - input graph\n",
    "    homes - list of vertices in `graph` that are marked as homes\n",
    "    source - vertex in `graph` that is the start and end of the path followed by the car\n",
    "    num_clusters - the number of clusters to be used to group the homes together\n",
    "    \"\"\"\n",
    "    homes_subgraph = tsp_routines.complete_shortest_path_subgraph(graph,homes)\n",
    "    home_clusters = k_cluster(homes_subgraph,num_clusters)\n",
    "    # The source vertex is added to each of the clusters before determining the best dropoff location.\n",
    "    # This is done so that vertices that are closer to the source are given higher preference as dropoff points.\n",
    "    add_vertex_to_clusters(home_clusters,source)\n",
    "    dropoff_vertices = get_dropoff_vertices(graph, home_clusters)\n",
    "    # Add the source to the dropoff vertices\n",
    "    dropoff_vertices.append(source)\n",
    "    # Get rid of any repeating entries in the dropoff vertices\n",
    "    dropoff_vertices = list(set(dropoff_vertices))\n",
    "    # Construct the fully connected sub-graph with the dropoff vertices \n",
    "    # on which TSP is computed\n",
    "    dropoff_subgraph = tsp_routines.complete_shortest_path_subgraph(graph,dropoff_vertices)\n",
    "    tsp_route = tsp_routines.metric_mst_tsp(dropoff_subgraph,source)\n",
    "    final_path = tsp_solution_to_path(graph,tsp_route)\n",
    "    return final_path\n",
    "    \n",
    "def graph_from_input(filename):\n",
    "    '''graph_from_input(str) --> nx.graph G, int source, np.ndarray[int] homes, dict locToIndex \n",
    "    Returns a graph created by reading the input file, with integer vertex labels\n",
    "    Returns list of the home indices\n",
    "    Returns a map from integer to the name associated with that node'''\n",
    "    with open(filename, 'r') as f:\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        locToIndex = {} # maps location name to its index number\n",
    "        homes = []\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        numLocations = int(lines[0])\n",
    "        numTAs = int(lines[1])\n",
    "        locations = lines[2].split()\n",
    "    \n",
    "        i = 0\n",
    "        assert len(locations) == numLocations, \"Number of locations must match specified value\"\n",
    "        for loc in locations:\n",
    "            G.add_node(i)\n",
    "            locToIndex[loc] = i\n",
    "            i += 1\n",
    "            \n",
    "        TAhomes = lines[3].split()\n",
    "        assert len(TAhomes) == numTAs, \"Number of TA homes must match specified value\"\n",
    "        for home in TAhomes:\n",
    "            homes.append(locToIndex[home])\n",
    "        \n",
    "        source = locToIndex[lines[4].strip()]\n",
    "        \n",
    "        row = 0\n",
    "        for line in lines[5:]:\n",
    "            line = line.split()\n",
    "            for col in range(len(line)):\n",
    "            \n",
    "                if line[col] != 'x':  \n",
    "                    G.add_edge(row, col)\n",
    "                    weight = float(line[col])\n",
    "                    G[row][col]['weight'] = weight\n",
    "            row += 1\n",
    "        \n",
    "        indexToLoc = {v: k for k, v in locToIndex.items()}\n",
    "        return G, source, homes, indexToLoc\n",
    "    \n",
    "def write_output_file(path, dropOffs, indexToName, filename):\n",
    "    '''path is a list of integers that we follow in the car \n",
    "    dropOffs is a dictionary (int -> [home, home, ...] ) mapping nodes to the homes of the TAs that get off at that node \n",
    "    indexToName is a list of names corresponding to each index\n",
    "    filename is a string filename that we write to  \n",
    "    '''\n",
    "    with open(filename, 'w') as f:\n",
    "        for step in path:\n",
    "            f.write(indexToName[step] + \" \")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        count = 0\n",
    "        for key in dropOffs:\n",
    "            if dropOffs[key] != []:\n",
    "                count += 1\n",
    "        f.write(str(count) + \"\\n\")\n",
    "        \n",
    "        for key in dropOffs:\n",
    "            if dropOffs[key] != []:\n",
    "                f.write(indexToName[key] + \" \")\n",
    "                for elt in dropOffs[key]:\n",
    "                    f.write(indexToName[elt] + \" \");\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "def eval_cost(G, path, dropoffs):\n",
    "    '''\n",
    "    eval_cost(nx.graph, np.ndarray, dict) -> int\n",
    "    path is a iterable of integers that we follow in the car \n",
    "    dropOffs is a dictionary (int -> [home, home, ...] )\n",
    "    '''\n",
    "    assert isinstance(G, nx.Graph) , \"G must be a graph\"\n",
    "    assert hasattr(path, '__iter__') , \"path must be an array of integers\"\n",
    "    assert isinstance(dropoffs, dict) , \"dropoffs must be a dictionary mapping node to homes\"\n",
    "    \n",
    "    cost = 0\n",
    "    prevNode = path[0]\n",
    "    for node in path[1:]:\n",
    "        cost += 2/3 * G[prevNode][node]['weight'] # weight of the edge from previous to next node\n",
    "        prevNode = node\n",
    "        \n",
    "        for home in dropoffs[node]:\n",
    "            cost += nx.astar_path_length(G, node, home)\n",
    "                \n",
    "    return cost\n",
    "\n",
    "def nearest_dropoff(G,route,homes):\n",
    "    \"\"\"\n",
    "    Returns a list representing the nearest dropoff point for each home\n",
    "    G - graph with weights specified as distances\n",
    "    route - array of indices representing vertices on path of Car\n",
    "    homes - array of indices representing vertices which are TA homes\n",
    "    \"\"\"\n",
    "    shortest_path_lens = dict(nx.all_pairs_dijkstra_path_length(G))\n",
    "    \n",
    "    wheredict = dict()\n",
    "    for node in route:\n",
    "        wheredict.update({node:[]}) #initialize a dropoff at each node in route. there could be places where noone is dropped.\n",
    "    \n",
    "    for h in homes:\n",
    "        ls = shortest_path_lens[h]#shortest path from h to all nodes (dict)\n",
    "        ls = {k:ls[k] for k in route}\n",
    "        wheredict[min(ls, key=ls.get)].append(h)\n",
    "    \n",
    "    return wheredict\n",
    "\n",
    "def solver(G, homes, source, k):\n",
    "    \"\"\"returns cost, dropoffs, route for a given k\"\"\"\n",
    "    route = solve_by_clustering(G, homes, source, k)\n",
    "    dropoffs = nearest_dropoff(G, route, homes)\n",
    "    cost = eval_cost(G, route, dropoffs)\n",
    "    return cost, dropoffs, route\n",
    "\n",
    "def itersolver(G, homes, source, verbosity = 1):\n",
    "    \"\"\"starts with nclusters as len(homes), divides by 2 and iteratively solves halving each time\"\"\"\n",
    "    iterations = int(np.log(len(homes))/np.log(2))\n",
    "    i = 0\n",
    "    ks = np.array([int(k) for k in np.linspace(1, len(homes), len(homes))])\n",
    "    workerk = ks[-1]\n",
    "    nextk = int(np.median(ks))\n",
    "    cost, dropoffs, route = solver(G, homes, source, workerk)\n",
    "    outcost = cost\n",
    "    outdropoffs, outroute = dropoffs, route\n",
    "    while i<iterations:\n",
    "        ncost, ndropoffs, nroute = solver(G, homes, source, nextk)\n",
    "        if outcost<ncost:\n",
    "            ks = ks[ks>nextk]\n",
    "            nextk = int(np.median(ks))\n",
    "            if verbosity > 2:\n",
    "                print('top, next - ', nextk)\n",
    "        else:\n",
    "            ks = ks[ks<=nextk]\n",
    "            outcost, outdropoffs, outroute = ncost, ndropoffs, nroute\n",
    "            nextk = int(np.median(ks))\n",
    "            if verbosity > 2:\n",
    "                print('bottom, next - ', nextk)\n",
    "        i+=1\n",
    "    return outcost, outdropoffs, outroute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_itersolver(fnames, verbosity = 1, redo = False):\n",
    "    \"\"\"run the solver on your set. save to file. return dict of fname and cost\n",
    "    \n",
    "    fnames - list of names to run on\n",
    "    if redo then it will recalculate ones which have already been calculated\n",
    "    \"\"\"\n",
    "    outdict = dict()\n",
    "    n = len(fnames)\n",
    "    for lmn, fname in enumerate(fnames):\n",
    "        if not redo:\n",
    "            if fname[:-3]+'.out' in set(os.listdir('./outputs')):\n",
    "                print(fname[:-3], 'already processed. skipping')\n",
    "                continue\n",
    "        if verbosity >1:\n",
    "            sys.stdout.write(\"\\rProcessing input %i of \"% lmn + str(n))\n",
    "            sys.stdout.flush()\n",
    "        G, source, homes, node_to_name_map = graph_from_input('./inputs/'+fname)\n",
    "        cost, dropoffs, route = itersolver(G, homes, source, verbosity = verbosity)\n",
    "        write_output_file(route, dropoffs, node_to_name_map, './outputs/'+fname[:-3]+'.out')\n",
    "        outdict.update({fname:cost})\n",
    "    return outdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./inputlist.pkl', 'wb') as f:\n",
    "#    pickle.dump(inputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./inputlist.pkl', 'rb') as f:\n",
    "    inputs = pickle.load(f)\n",
    "fnames_eric = inputs[:316]\n",
    "fnames_saagar = inputs[316:632]\n",
    "fnames_arjun = inputs[632:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costdict = run_itersolver(fnames_eric,redo = False,verbosity = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
