{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import input_functions as inp\n",
    "import tsp_routines\n",
    "from clustering.funcs import k_cluster\n",
    "from clustering.funcs import best_dropoff\n",
    "from input_functions.funcs import create_new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_from_input(filename):\n",
    "    '''graph_from_input(str) --> nx.graph G, np.ndarray[int] homes, dict locToIndex \n",
    "    Returns a graph created by reading the input file, with integer vertex labels\n",
    "    Returns list of the home indices\n",
    "    Returns a map from integer to the name associated with that node'''\n",
    "    with open(filename, 'r') as f:\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        locToIndex = {} # maps location name to its index number\n",
    "        homes = []\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        numLocations = int(lines[0])\n",
    "        numTAs = int(lines[1])\n",
    "        locations = lines[2].split()\n",
    "    \n",
    "        i = 0\n",
    "        assert len(locations) == numLocations, \"Number of locations must match specified value\"\n",
    "        for loc in locations:\n",
    "            G.add_node(i)\n",
    "            locToIndex[loc] = i\n",
    "            i += 1\n",
    "            \n",
    "        TAhomes = lines[3].split()\n",
    "        assert len(TAhomes) == numTAs, \"Number of TA homes must match specified value\"\n",
    "        for home in TAhomes:\n",
    "            homes.append(locToIndex[home])\n",
    "        \n",
    "        homes.insert(0, locToIndex[lines[4].strip()])\n",
    "        \n",
    "        row = 0\n",
    "        for line in lines[5:]:\n",
    "            line = line.split()\n",
    "            for col in range(len(line)):\n",
    "            \n",
    "                if line[col] != 'x':  \n",
    "                    G.add_edge(row, col)\n",
    "                    weight = float(line[col])\n",
    "                    G[row][col]['weight'] = weight\n",
    "            row += 1\n",
    "            \n",
    "        indexToLoc = {v: k for k, v in locToIndex.items()}\n",
    "        return G, homes, indexToLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cost(G, path, dropoffs):\n",
    "    '''\n",
    "    eval_cost(nx.graph, np.ndarray, dict) -> int\n",
    "    path is a list of integers that we follow in the car \n",
    "    dropOffs is a dictionary (int -> [home, home, ...] )\n",
    "    '''\n",
    "    assert isinstance(G, nx.Graph) , \"G must be a graph\"\n",
    "    assert isinstance(path, np.ndarray) , \"path must be an array of integers\"\n",
    "    assert isinstance(dropoffs, dict) , \"dropoffs must be a dictionary mapping node to homes\"\n",
    "    \n",
    "    cost = 0\n",
    "    prevNode = path[0]\n",
    "    for node in path[1:]:\n",
    "        cost += 2/3 * G[prevNode][node]['weight'] # weight of the edge from previous to next node\n",
    "        prevNode = node\n",
    "        \n",
    "        for home in dropoffs[node]:\n",
    "            cost += nx.astar_path_length(G, node, home)\n",
    "                \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dict_entries(input_dict,nodemapper):\n",
    "    \"\"\"\n",
    "    maps the keys and values of the input dict using the nodemapper dict.\n",
    "    \"\"\"\n",
    "    output_dict = dict()\n",
    "    for key in input_dict.keys():\n",
    "        mapped_key = nodemapper[key]\n",
    "        output_dict.update({mapped_key:[]})\n",
    "        for vertex in input_dict[key]:\n",
    "            output_dict[mapped_key].append(nodemapper[vertex])\n",
    "    return output_dict\n",
    "\n",
    "def add_vertex_to_clusters(clusters,vertex):\n",
    "    \"\"\"\n",
    "    add the given vertex to each cluster.\n",
    "    Input:\n",
    "    clusters - dict where the keys are vertices which are cluster centers and the values are a list of \n",
    "                vertices belonging to this cluster\n",
    "    vertex - the vertex to be added to each list in `clusters`\n",
    "    \"\"\"\n",
    "    for key in clusters:\n",
    "        clusters[key].append(vertex)\n",
    "        \n",
    "def get_dropoff_vertices(clusters):\n",
    "    best_dropoffs = []\n",
    "    for key in clusters:\n",
    "        dropoff = best_dropoff(G,clusters[key])\n",
    "        best_dropoffs.append(dropoff)\n",
    "    return best_dropoffs\n",
    "        \n",
    "def solve_by_clustering(graph,homes,source,num_clusters):\n",
    "    \"\"\"\n",
    "    return the route to be followed by the car as it drops off TAs.\n",
    "    Inputs:\n",
    "    graph - input graph\n",
    "    homes - list of vertices in `graph` that are marked as homes\n",
    "    source - vertex in `graph` that is the start and end of the path followed by the car\n",
    "    num_clusters - the number of clusters to be used to group the homes together\n",
    "    \"\"\"\n",
    "    homes_subgraph = tsp_routines.complete_shortest_path_subgraph(graph,homes)\n",
    "    home_clusters = k_cluster(homes_subgraph,num_clusters)\n",
    "    # The source vertex is added to each of the clusters before determining the best dropoff location.\n",
    "    # This is done so that vertices that are closer to the source are given higher preference as dropoff points.\n",
    "    add_vertex_to_clusters(home_clusters,source)\n",
    "    dropoff_vertices = get_dropoff_vertices(home_clusters)\n",
    "    # Add the source to the dropoff vertices\n",
    "    dropoff_vertices.append(source)\n",
    "    # Get rid of any repeating entries in the dropoff vertices\n",
    "    dropoff_vertices = list(set(dropoff_vertices))\n",
    "    # Construct the fully connected sub-graph with the dropoff vertices \n",
    "    # on which TSP is computed\n",
    "    dropoff_subgraph = tsp_routines.complete_shortest_path_subgraph(graph,dropoff_vertices)\n",
    "    tsp_route = tsp_routines.metric_mst_tsp(dropoff_subgraph,source)\n",
    "    return tsp_route\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cost(G, route, homes):\n",
    "    carcost = 0\n",
    "    workingnode = route[0]\n",
    "    for node in route[1:]:\n",
    "        carcost+=(2/3)*nx.dijkstra_path_length(G, workingnode, node)\n",
    "        workingnode = node\n",
    "        \n",
    "    ta_cost = 0\n",
    "    where_get_off = dict()\n",
    "    for home in homes:\n",
    "        local_cost = np.inf\n",
    "        for node in route:\n",
    "            l = nx.dijkstra_path_length(G, home, node)\n",
    "            if l < local_cost:\n",
    "                local_cost = l\n",
    "                bestnode = node\n",
    "            else:\n",
    "                continue\n",
    "        ta_cost += local_cost\n",
    "        try:\n",
    "            where_get_off[bestnode].append(home)\n",
    "        except KeyError:\n",
    "            where_get_off.update({bestnode:[home]})\n",
    "    return carcost, where_get_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_solver(filename,kiterations = 'all'):\n",
    "    \"\"\"returns the best route and its cost and where each ta gets off, found by decreasing nclusters from nhomes to 2\"\"\"\n",
    "    G, homes, node_to_name_map = graph_from_input(filename)\n",
    "    k = len(homes) #how many clusters initially\n",
    "    cost = np.inf\n",
    "    out_route = []\n",
    "    if kiterations == 'all':\n",
    "        krange = range(k-2)\n",
    "    else:\n",
    "        krange = range(kiterations)\n",
    "    for i in krange:\n",
    "        #assumed that homes[0] is the source\n",
    "        tsp_route = solve_by_clustering(G,homes,homes[0],k)\n",
    "        tempcost, where_get_off = eval(G, tsp_route, homes)\n",
    "        if tempcost<cost:\n",
    "            cost = tempcost\n",
    "            out_route = tsp_route\n",
    "            dropoffloc_dict = where_get_off\n",
    "        k-=1\n",
    "    return out_route, cost, dropoffloc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './inputs/20_50.in'\n",
    "G, homes, node_to_name_map = graph_from_input(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "route, cost, dropoffs = cluster_solver(filename, kiterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp_route = solve_by_clustering(G,homes[1:],homes[0],26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.has_edge(37,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 10, 7, 19, 29, 37, 13, 8, 11, 23, 36, 9, 46, 41, 32, 31, 24, 6, 47, 42, 2, 34, 17, 25, 1, 27, 21] 5163591867.49498\n"
     ]
    }
   ],
   "source": [
    "print(route, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 35, 13]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.dijkstra_path(G,37, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "13",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-13d01adc7846>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropoffs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-653b6cca0040>\u001b[0m in \u001b[0;36meval_cost\u001b[1;34m(G, path, dropoffs)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprevNode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprevNode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# weight of the edge from previous to next node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mprevNode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\epars\\Anaconda3\\lib\\site-packages\\networkx\\classes\\coreviews.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_atlas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 13"
     ]
    }
   ],
   "source": [
    "eval_cost(G, np.array(route), dropoffs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
